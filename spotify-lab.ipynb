{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spotify Login - For One Hour\n",
    "After running this click Sign-In and login to Spotify. If the Time expires then repeat the process again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipyauth import ParamsSpotify, Auth\n",
    "\n",
    "auth = Auth(ParamsSpotify(redirect_uri='http://localhost:8888/callback', client_id=\"9e4657eefbac41afa98c61f590d8fd51\"))\n",
    "auth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common Stuff and Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from IPython.display import Image\n",
    "from pandas.io.json import json_normalize\n",
    "from pandas import DataFrame,read_pickle\n",
    "import logging\n",
    "import os\n",
    "\n",
    "logger = logging.getLogger()\n",
    "#logger.setLevel(logging.INFO)\n",
    "\n",
    "def fetch( path, url=None ):\n",
    "    callPath = url if (url!=None) else (\"https://api.spotify.com\" + path)\n",
    "    response = requests.get(callPath , headers= {\"Authorization\":\"Bearer \" + auth.access_token })\n",
    "    if (response.status_code!=200):\n",
    "        logging.debug(\"error \")\n",
    "    return response.json()\n",
    "\n",
    "def fetchPage( path, offset, limit):\n",
    "    res = fetch(  path + \"?offset=\" + str(offset) + \"&limit=\" + str(limit))\n",
    "    return res\n",
    "\n",
    "def fetchAll( path ):\n",
    "    more = fetchPage( path, 0, 50)\n",
    "    limit = more[\"limit\"]\n",
    "    total = more[\"total\"] - limit\n",
    "    items = more[\"items\"]\n",
    "    while((total>0) and (more[\"next\"]!=None )):\n",
    "        more = fetch( None, url = more[\"next\"] )\n",
    "        items.extend( more[\"items\"])\n",
    "        total = total - len(more[\"items\"])\n",
    "    return items\n",
    "\n",
    "def fetchPageIds( path, ids): \n",
    "  res = fetch(\"{0}?ids={1}\".format(path,\",\".join(ids)))\n",
    "  return res\n",
    "\n",
    "def fetchAllIds( path, resultField, ids, pageSize=50, existingDf=None):\n",
    "    if (existingDf is not None):\n",
    "        keys = existingDf.index.values\n",
    "        ids = list(filter( lambda id: (id not in keys),ids))\n",
    "        existingDf = existingDf.reset_index()\n",
    "        logging.info(\"Filtering out {0} IDs. Now {1}\".format(len(keys), len(ids)))\n",
    "    total = len(ids)\n",
    "    logging.info(\"Requesting {0} rows. {1} ... {2}\".format(total, path, resultField))\n",
    "    offset = 0\n",
    "    while (offset < total) :\n",
    "        result = fetchPageIds(path, ids[offset: min(total, offset + pageSize)])\n",
    "        items = json_normalize(result[resultField])\n",
    "        if (existingDf is None):\n",
    "            logging.info(\"Creating new DF {0}\".format(len(items)))\n",
    "            existingDf = items\n",
    "        else:\n",
    "            existingDf = existingDf.append(items, ignore_index=True )\n",
    "        offset += len(items)\n",
    "        \n",
    "    return existingDf.to_dict(orient=\"records\")\n",
    "\n",
    "user = fetch(\"/v1/me\")\n",
    "Image(url=user[\"images\"][0][\"url\"], width=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read The User Library\n",
    "This process may take a little while. The library tracks are cached locally, so this step can be skipped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = fetchAll(\"/v1/me/tracks\")\n",
    "tracksDf = json_normalize(data, sep=\"_\").set_index(\"track_id\")\n",
    "tracksDf.to_pickle(\"mytracks.pkl\")\n",
    "tracksDf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify the library cache exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tracksDf = read_pickle(\"mytracks.pkl\")\n",
    "tracksCache = tracksDf[[\"added_at\",\"track_name\",\"track_album_name\"]]\n",
    "tracksCache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Track to Artist Table\n",
    "Pick out the track.artists array for each library track record. The meta (parent record) is the track.id. Use a prefix for both the meta and the record because they both use id.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data[0]\n",
    "artist_and_track = json_normalize( data=data, record_path=['track','artists'],  meta=[[\"track\",\"id\"],[\"track\",\"name\"]],  record_prefix='artist_',   sep=\"_\" )\n",
    "artist_and_track = artist_and_track[['track_id','track_name','artist_id','artist_name']]\n",
    "artist_and_track"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the Artists\n",
    "Load the artists one by one. Use the Pickle File **artists.pkl** as a cache.\n",
    "* Create a DataFrame using the cache file if one exists (otherwise None)\n",
    "* Get the list of all unique artist_ids from the previous DF\n",
    "* Call `fetchAllIds` - using the artists path, the `artists` JSON field path and the cache DF\n",
    "* Recreate the new artistDf (Dict returned)\n",
    "* Save the file back to **artists.pkl**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "artistsPickle = read_pickle(\"artists.pkl\") if (os.path.isfile(\"artists.pkl\")) else None \n",
    "artistIds = list(set(artist_and_track[\"artist_id\"].values))\n",
    "artists = fetchAllIds(\"/v1/artists\",\"artists\",artistIds,existingDf=artistsPickle)\n",
    "artistsDf = json_normalize(artists).set_index(\"id\")\n",
    "artistsDf.to_pickle(\"artists.pkl\")\n",
    "artistsDf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the Albums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "albumsPickle = read_pickle(\"albums.pkl\") if (os.path.isfile(\"albums.pkl\")) else None \n",
    "album_ids  = list(set(json_normalize( data=data, sep=\"_\" )[\"track_album_id\"].values))\n",
    "albums = fetchAllIds(\"/v1/albums\",\"albums\",album_ids,pageSize=20,existingDf=albumsPickle)\n",
    "albumsDf = json_normalize(albums, sep=\"_\").set_index(\"id\")\n",
    "albumsDf.to_pickle(\"albums.pkl\")\n",
    "#albumsDf.columns\n",
    "albumsDf[[\"name\",\"release_date\",\"tracks.total\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
